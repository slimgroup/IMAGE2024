---
title: "Enhancing Full-Waveform Variational Inference through Stochastic Resampling and Data Augmentation"
author:
  - name: Yunlin Zeng^1^, Rafael Orozco,^1^, Ziyi Yin^1^, Felix Herrmann ^1^ 
format: 
  pdf:
    number-sections: true
bibliography: image2024.bib
---

**Introduction** Full Waveform Inversion (FWI) is a computationally expensive iterative optimization technique that determines the optimal subsurface velocity model by minimizing the discrepancy between observed seismic data and data generated by a forward model. Recent advancements, such as amortized FWI, have enabled fast online inference following extensive offline training on amortized variational inference (i.e., $p_\theta (x|y)$, where $x$ is velocity and $y$ is seismic data). Additionally, the standard deviation of posterior samples serves as a useful metric for uncertainty quantification. The integration of physics-informed summary statistics, such as reverse-time migration (RTM) and common image gather (CIG), as data $y$ significantly enhances the computational efficiency of conditional normalizing flow (CNF) training. Both RTM and CIG rely on accurate initial velocity models ($x_0$). This study investigates how varying initial backgrounds influence the quality of the amortized sampler. 

**Method** We introduce a novel data augmentation method to train amortized posterior samplers in the context of simulation-based inference. By interpreting the imaging background model as a stochastic latent variable we propose to resample this during training in order to (1) Increase the robustness of the inference result wrt to the background model used at test time and (2) Expand the dataset by perturbing and smoothing out the velocity $x$, utilizing the outcomes as new migration background models. This approach enables the computation of new RTM based on these models, thereby increasing the number of $\{x, \text{RTM}\}$ training pairs.  

Following [@yin2023wise], we trained a CNF that takes RTM as input and outputs a velocity model. While replicating their training, we enhance the posterior sampler's accuracy by perturbing and augmenting the velocity dataset. We generate $x_0$ by perturbing a ground truth velocity model and applying smoothing.  Initially, we train a baseline CNF using $800$ velocity-RTM pairs, with $x_0$ derived by perturbing the true velocity data $x$ using the proposed  method above, generating additional RTM data through migration. The efficacy of this approach is compared against simply repeating the $\{x, \text{RTMs}\}$ pairs.

**Result** Utilizing 2D slices of the compass dataset [@Jones2012], we generate velocity-RTM training pairs through the aforementioned approach. The dataset is divided, allocating $800$ pairs for training and $150$ for testing. By perturbing the velocity models an additional one to three times, we augment the velocity-RTM training dataset to $1600$, $2400$, and $3200$ pairs, respectively. Conversely, we replicate the baseline dataset two to four times to match the augmented dataset sizes. The CNFs trained with these varied datasets are evaluated by structural similarity index measure (SSIM) and root mean square error (RMSE) against the ground truth. The results show that posterior samples progressively align more closely with the ground truth and reduce uncertainty as the dataset augmentation size increases. Furthermore, they also demonstrate that the augmented datasets consistently outperform their repetitive counterparts.

**Novelty** The success of our approach is rooted in the innovative reinterpretation of the imaging background model as a stochastic latent variable, which we resample during training. This strategy serves two pivotal functions: firstly, it significantly enhances the robustness of our inference results wrt the background model at test time, ensuring reliability in diverse scenarios. Secondly, by capturing the variabilities from perturbing and smoothing velocity as new migration background models, we not only broaden the scope of our model's applicability but also directly contribute to the improved precision and decreased uncertainty of our posterior estimates. 

## Reference
