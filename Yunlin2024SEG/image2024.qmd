---
title: "Enhancing Full-Waveform Variational Inference through Stochastic Resampling and Data Augmentation"
author:
  - name: Yunlin Zeng^1^, Rafael Orozco,^1^, Ziyi Yin^1^, Felix Herrmann ^1^ 
bibliography: image2024.bib
---

<!-- 
format: 
  pdf:
    number-sections: true -->

**Introduction:** Full Waveform Inversion (FWI) is a computationally expensive iterative optimization scheme that determines migration-velocity models by minimizing the discrepancy between observed seismic shot data and data generated by a forward model parameterized by the velocity. Recent advancements, such as full-Waveform variational Inference via Subsurface Extensions (WISE), produce  amortized neural posterior estimates that enable fast online inference. These neural approximations to the posterior distribution, $p_{\boldsymbol{\theta}}(\mathbf{x}\vert\mathbf{y})\approx p(\mathbf{x}\vert\mathbf{y})$ with $\mathbf{x}$, the velocity model, $\mathbf{x}$, the shot data, and $\boldsymbol{\theta}$, the network weights, are obtained with variational inference, which requires extensive off-line training. Aside from providing statistically robust estimates via the conditional mean, $\mathbb{E}_\mathbf{x}(p_{\boldsymbol{\theta}}(\mathbf{x}\vert\mathbf{y}))$ these neural posterior also provide a useful metric of the uncertainty in terms of the variance $\mathbb{V}_\mathbf{x}(p_{\boldsymbol{\theta}}(\mathbf{x}\vert\mathbf{y}))$. To make this approach computationally feasible, we follow @yin2023wise and train on pairs $\{(\mathbf{x}^{(m)}, \mathbf{\bar{y}}^{(m)})\}_{m=1}^M$, where the $\mathbf{\bar{y}}^{(m)}$ stand for subsurface-offset Common Image Gathers, computed from each shot dataset, $\mathbf{y}^{(m)}$. While CIG's as *physics-based summary statistics* are better suited to inform the posterior than plane migration---i.e, they preserve information even when the migration-velocity model is poor, their performance still depends on the the choice of the initial migration-velocity model, $\mathbf{x}_0$. In During this talk, we will study the impact of varying initial background velocity models on the quality of the amortized neural posterior sampler. We will also investigate how augmenting the training set with different initial background velocity models can lead to improved performance.

**Method:** By interpreting the initial migration-velocity model as a stochastic latent variable, we propose to augment the training pairs, $\{(\mathbf{x}^{(m)}, \mathbf{\bar{y}}^{(m)})\}_{m=1}^M$, where the $\mathbf{\bar{y}}^{(m)}$ with the $\mathbf{\bar{y}}^{(m)$'s computed with one single initial migration-velocity model, $\mathbf{x}_0$, with $\mathbf{\bar{y}}^{(m)$ obtained with different (perturbed) initial migration-velocity model, $\mathbf{x}_0$. The aim of this training dataset augmentation is to improve the robustness of WISE---i.e., make its neural posterior estimation less dependent on the choice for the initial migration-velocity model, $\mathbf{x}_0$. 

**Results:** To improve the neural posterior estimation's robustness, multiple initial migration-velocity models, $\mathbf{x}_0$, are obtained  by perturbing the ground-truth velocity models, $\mathbf{x}$, followed by extensive smoothing. For comparison, we train the baseline CNF on $M=800$ velocity-extended image pairs, fir a single $\mathbf{x}_0$. This baseline is compared with XXX explain use text below....

 Utilizing 2D slices of the compass dataset [@Jones2012], we generate velocity-RTM training pairs through the aforementioned approach. The dataset is divided, allocating $800$ pairs for training and $150$ for testing. By perturbing the velocity models an additional one to three times, we augment the velocity-RTM training dataset to $1600$, $2400$, and $3200$ pairs, respectively. Conversely, we replicate the baseline dataset two to four times to match the augmented dataset sizes. The CNFs trained with these varied datasets are evaluated by structural similarity index measure (SSIM) and root mean square error (RMSE) against the ground truth. The results show that posterior samples progressively align more closely with the ground truth and reduce uncertainty as the dataset augmentation size increases. Furthermore, they also demonstrate that the augmented datasets consistently outperform their repetitive counterparts.

**Novelty** The success of our approach is rooted in the innovative reinterpretation of the imaging background model as a stochastic latent variable, which we resample during training. This strategy serves two pivotal functions: firstly, it significantly enhances the robustness of our inference results wrt the background model at test time, ensuring reliability in diverse scenarios. Secondly, by capturing the variabilities from perturbing and smoothing velocity as new migration background models, we not only broaden the scope of our model's applicability but also directly contribute to the improved precision and decreased uncertainty of our posterior estimates. 

## Reference
