---
title: "Enhancing Full-Waveform Variational Inference through Stochastic Resampling and Data Augmentation"
author:
  - name: 
      Yunlin Zeng^1^, Rafael Orozco,^1^, Ziyi Yin^1^, Felix Herrmann ^1^ \ 
       ^1^ Georgia Institute of Technology
  
bibliography: paper.bib
---


**Introduction** Full Waveform Inversion (FWI) is a computationally expensive iterative optimization scheme that determines migration-velocity models by minimizing the discrepancy between observed seismic shot data and data generated by a forward model parameterized by the velocity. Recent advancements, such as full-Waveform variational Inference via Subsurface Extensions (WISE), produce  amortized neural posterior estimates that enable fast online inference. These neural approximations to the posterior distribution, $p_{\boldsymbol{\theta}}(\mathbf{x}\vert\mathbf{y})\approx p(\mathbf{x}\vert\mathbf{y})$ with $\mathbf{x}$, the velocity model, $\mathbf{y}$, the shot data, and $\boldsymbol{\theta}$, the network weights, are obtained with variational inference, which requires extensive off-line training. Aside from providing statistically robust estimates via the conditional mean, $\mathbb{E}_{\mathbf{x}\sim p_{\boldsymbol{\theta}}(\mathbf{x}\vert\mathbf{y})}(\mathbf{x})$ these neural posterior also provide a useful metric of the uncertainty in terms of the variance $\mathbb{V}_{\mathbf{x}\sim p_{\boldsymbol{\theta}}(\mathbf{x}\vert\mathbf{y})}(\mathbf{x})$. To make this approach computationally feasible, we follow @yin2023wise and train on pairs $\{(\mathbf{x}^{(m)}, \mathbf{\bar{y}}^{(m)})\}_{m=1}^M$, where the $\mathbf{\bar{y}}^{(m)}$ stand for subsurface-offset Common Image Gathers, computed from each shot dataset, $\mathbf{y}^{(m)}$. While CIG's as *physics-based summary statistics* are better suited to inform the posterior than plane migration---i.e, they preserve information even when the migration-velocity model is poor, their performance still depends on the the choice of the initial migration-velocity model, $\mathbf{x}_0$. During this talk, we will study the impact of varying initial background velocity models on the quality of the amortized neural posterior sampler. We will also investigate how augmenting the training set with different initial background velocity models can lead to improved performance.

**Method** By interpreting the initial migration-velocity model as a stochastic latent variable, we propose to augment the training pairs, $\{(\mathbf{x}^{(m)}, \mathbf{\bar{y}}^{(m)})\}_{m=1}^M$, where the $\mathbf{\bar{y}}^{(m)}$ with the $\mathbf{\bar{y}}^{(m)}$'s computed with one single initial migration-velocity model, $\mathbf{x}_0$, with $\mathbf{\bar{y}}^{(m)}$ obtained with different (perturbed) initial migration-velocity model, $\mathbf{x}_0$. The aim of this training dataset augmentation is to improve the robustness of WISE---i.e., make its neural posterior estimation less dependent on the choice for the initial migration-velocity model, $\mathbf{x}_0$. 

**Results** To improve the neural posterior estimation's robustness, multiple initial migration-velocity models, $\mathbf{x}_0$, are obtained by perturbing the ground-truth velocity model, $\mathbf{x}$, followed by extensive smoothing. Since this deformation is stochastic we can generate an infinite number of background models to use during training, although in practice we regenerate the background model a few times (3-5). Figure \ref{fig:workflow}(a)-(c) shows the complete framework.

To evaluate our method, we divide the compass model dataset [@Jones2012] by allocating $800$ pairs for training and $150$ for testing. We train the baseline CNF on $M=800$ velocity-extended image pairs, and train enhanced CNFs on augmented training pairs ($M=1600, 2400, ...$ etc.). The trained CNFs are evaluated by structural similarity index measure (SSIM) and root mean square error (RMSE) of the posterior samples' mean against the ground truth. The results in Figure \ref{fig:workflow}(d) show that posterior samples progressively align more closely with the ground truth. We also observe the trend that the uncertainty is reduced as we increase the number of background model regenerations used during training. This implies that the uncertainty information in the baseline has not properly captured the uncertainty due to the background model while our method has learned to incorporate this uncertainty information into the final inference result.

::: {#fig-framwork}
![](./figs/framework.png){width="90%"}

The complete stochastic resampling framework: (a) Perturb and smooth out velocity data to get new background model $\mathbf{x}_0$. (b) Generate $\mathbf{\bar{y}}$ based on $\mathbf{x}_0$ and seismic shot data $\mathbf{y}$. (c) Train the CNF using $\{\mathbf{x}, \mathbf{\bar{y}}\}$ pairs. (d) Comparison of conditional mean and errors against the ground truth velocity between the enhanced CNF and the baseline CNF.
:::

## Reference
